{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d54599f",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4a87224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from JL_baseline import StreamingJL\n",
    "from StreamingANN import StreamingANN\n",
    "import psutil, os, time, json\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc5dba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_fvecs(fname):\n",
    "    # Read raw data as int32 (because first value of each record is dimension)\n",
    "    fv = np.fromfile(fname, dtype=np.int32)\n",
    "    d = fv[0]  # first entry is the dimension\n",
    "    fv = fv.reshape(-1, d + 1).astype(np.float32)\n",
    "    return fv[:, 1:]  # drop the dimension prefix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d645362",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_fvecs('data/sift_base.fvecs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c57f0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0.39071167  Max: 1.3215674  Mean: 0.8274192\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Normalize first 5000 dataset points\n",
    "mat = np.vstack([df[i].astype(np.float32) for i in range(5000)])\n",
    "mat_norm = mat \n",
    "\n",
    "# Normalize query (first point)\n",
    "q = df[5257].astype(np.float32)\n",
    "q_norm = q\n",
    "\n",
    "# Compute distances\n",
    "dists = np.linalg.norm(mat_norm - q_norm, axis=1)\n",
    "\n",
    "print(\"Min:\", dists.min(), \" Max:\", dists.max(), \" Mean:\", dists.mean())\n",
    "\n",
    "# this tells us that r should be about 0.5 - otherwise trivial :( "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f52fb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80000, 384)\n"
     ]
    }
   ],
   "source": [
    "files = [\"data/encodings.npy\", \"data/encodings_2.npy\", \"data/encodings_3.npy\", \"data/encodings_4.npy\"]\n",
    "arrays = [np.load(f) for f in files]\n",
    "data = np.vstack(arrays)  \n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f20d7f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "print(data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31a571cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(1.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "724198f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined array to data/encodings_combined.npy\n"
     ]
    }
   ],
   "source": [
    "np.save(\"data/encodings_combined.npy\", data)\n",
    "print(\"Saved combined array to data/encodings_combined.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bafd8cf",
   "metadata": {},
   "source": [
    "We precompute and store all the nearest neighbours for each point because that saves us a lot of time while running experiments over the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b40a84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested r (inner radius) based on 5th percentile: 0.5158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float32(0.51577383)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def suggest_r(data, sample_size=10000, percentile=5, seed=42):\n",
    "    \"\"\"\n",
    "    Suggest a good inner radius `r` for StreamingANN / JL based on the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        Dataset array of shape (N, d)\n",
    "    sample_size : int\n",
    "        Number of random pairs to sample for estimating distances\n",
    "    percentile : float\n",
    "        Percentile of the distance distribution to pick as `r`\n",
    "        (e.g., 5 means the nearest 5% of points define \"close\")\n",
    "    seed : int\n",
    "        Random seed for reproducibility\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    r : float\n",
    "        Suggested radius\n",
    "    \"\"\"\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    N = len(data)\n",
    "    \n",
    "    # If dataset is small, compute all pairwise distances\n",
    "    if N*(N-1)//2 <= sample_size:\n",
    "        # Full pairwise distances\n",
    "        dists = np.linalg.norm(data[:, None, :] - data[None, :, :], axis=-1)\n",
    "        dists = dists[np.triu_indices(N, k=1)]\n",
    "    else:\n",
    "        # Sample random pairs\n",
    "        idx1 = rng.integers(0, N, size=sample_size)\n",
    "        idx2 = rng.integers(0, N, size=sample_size)\n",
    "        # Avoid identical points\n",
    "        mask = idx1 != idx2\n",
    "        idx1, idx2 = idx1[mask], idx2[mask]\n",
    "        dists = np.linalg.norm(data[idx1] - data[idx2], axis=1)\n",
    "\n",
    "    # Pick the desired percentile\n",
    "    r = np.percentile(dists, percentile)\n",
    "    print(f\"Suggested r (inner radius) based on {percentile}th percentile: {r:.4f}\")\n",
    "    return r\n",
    "\n",
    "suggest_r(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7dc831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fashion_mnist_csv(fname):\n",
    "    \"\"\"\n",
    "    Load Fashion-MNIST CSV file.\n",
    "    Assumes first column is label, remaining 784 columns are pixel values.\n",
    "    Scales pixel values to [0,1] and applies L2 normalization.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(fname)\n",
    "    data = df.iloc[:, 1:].to_numpy(dtype=np.float32)  # drop label column\n",
    "\n",
    "    # # Scale pixel values to [0,1]\n",
    "    # data /= 255.0\n",
    "\n",
    "    # Apply L2 normalization row-wise\n",
    "    norms = np.linalg.norm(data, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1.0  # avoid division by zero\n",
    "    data = data / norms\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fdac31e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_fashion_mnist_csv(\"data/fashion_mnist/fashion-mnist_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fb8bf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded nn_dict has 80000 entries\n"
     ]
    }
   ],
   "source": [
    "# Load it later\n",
    "with open(\"data/nn_dict.pkl\", \"rb\") as f:\n",
    "    loaded_nn_dict = pickle.load(f)\n",
    "\n",
    "print(\"Loaded nn_dict has\", len(loaded_nn_dict), \"entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79204eb9",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6308d818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58a3ccbc",
   "metadata": {},
   "source": [
    "### Evaluating the Basic Setup \n",
    "- Vary parameters r, epsilon, eta, n\n",
    "- For each query q, check if the returned nearest neighbor is within r, if YES, we must check if our algorithm returns a valid approximate neighbour\n",
    "- Want to check the basic scheme itself : how it performs (success rate) with increasing n, more aggressive sampling, and wider error margins "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cc74be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_completed_params(log_file):\n",
    "    \"\"\"Load completed (r, eps, eta, n, rep) tuples from a JSONL log file.\"\"\"\n",
    "    done = set()\n",
    "    if os.path.exists(log_file):\n",
    "        with open(log_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    rec = json.loads(line)\n",
    "                    key = (rec[\"r\"], rec[\"epsilon\"], rec[\"eta\"], rec[\"n\"], rec[\"repeat\"])\n",
    "                    done.add(key)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    return done\n",
    "\n",
    "\n",
    "def evaluate_streaming_ann(\n",
    "    data, nn_dict, r_list, eps_list, eta_list, n_list,\n",
    "    repeats=3, n_queries=10000, seed=42,\n",
    "    log_file=\"streaming_ann_log.jsonl\", checkpoint_every=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Run StreamingANN experiments with periodic checkpointing and resume support.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        Dataset array (N,d)\n",
    "    nn_dict : dict\n",
    "        Maps each vector index in data to its nearest neighbor\n",
    "    r_list, eps_list, eta_list, n_list : list\n",
    "        Parameter ranges\n",
    "    repeats : int\n",
    "        Number of repeats per configuration\n",
    "    n_queries : int\n",
    "        Fixed number of queries per run\n",
    "    log_file : str\n",
    "        Path to JSONL log file for checkpointing\n",
    "    checkpoint_every : int\n",
    "        Write to disk after every X results\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    proc = psutil.Process(os.getpid())\n",
    "    results_buffer = []\n",
    "    results = []\n",
    "\n",
    "    # Load completed experiments (resume support)\n",
    "    completed = load_completed_params(log_file)\n",
    "    print(f\"Found {len(completed)} completed runs in {log_file}, skipping them.\")\n",
    "\n",
    "    # Open log file in append mode\n",
    "    f = open(log_file, \"a\")\n",
    "\n",
    "    # total number of runs for progress bar\n",
    "    total_runs = len(r_list) * len(eps_list) * len(eta_list) * len(n_list) * repeats\n",
    "\n",
    "    with tqdm(total=total_runs, desc=\"StreamingANN experiments\") as pbar:\n",
    "        counter = 0\n",
    "        for r in r_list:\n",
    "            for eps in eps_list:\n",
    "                for eta in eta_list:\n",
    "                    for n in n_list:\n",
    "                        if n > len(data):\n",
    "                            continue\n",
    "                        for rep in range(repeats):\n",
    "                            key = (r, eps, eta, n, rep)\n",
    "                            counter += 1\n",
    "\n",
    "                            # Skip if already completed\n",
    "                            if key in completed:\n",
    "                                pbar.update(1)\n",
    "                                continue\n",
    "\n",
    "                            pbar.set_postfix({\n",
    "                                \"r\": r, \"eps\": eps, \"eta\": eta, \"n\": n, \"rep\": rep\n",
    "                            })\n",
    "\n",
    "                            # --- dataset and query split ---\n",
    "                            idx = rng.choice(len(data), size=n, replace=False)\n",
    "                            dataset = data[idx]\n",
    "                            remaining_idx = list(set(range(len(data))) - set(idx))\n",
    "                            q_count = min(n_queries, len(remaining_idx))\n",
    "                            q_idx = rng.choice(remaining_idx, size=q_count, replace=False)\n",
    "                            queries = data[q_idx]\n",
    "\n",
    "                            # --- memory snapshot before ANN build ---\n",
    "                            mem_before = proc.memory_info().rss\n",
    "\n",
    "                            # Build ANN\n",
    "                            ann = StreamingANN(d=data.shape[1], n_estimate=n,\n",
    "                                               eta=eta, epsilon=eps, r=r)\n",
    "                            for p in dataset:\n",
    "                                ann.insert(p)\n",
    "\n",
    "                            # --- memory snapshot after ANN build ---\n",
    "                            mem_after = proc.memory_info().rss\n",
    "                            mem_ann_mb = (mem_after - mem_before) / 1024**2\n",
    "\n",
    "                            # --- queries ---\n",
    "                            successes = 0\n",
    "                            valid_queries = 0\n",
    "                            query_times = []\n",
    "\n",
    "                            for q_global_idx, q in zip(q_idx, queries):\n",
    "                                # lookup precomputed nearest neighbor index\n",
    "                                nn_idx = nn_dict[q_global_idx]\n",
    "                                nn_point = data[nn_idx]\n",
    "\n",
    "                                # check if true neighbor is within radius r\n",
    "                                if np.linalg.norm(nn_point - q) <= r:\n",
    "                                    valid_queries += 1\n",
    "                                    t0 = time.perf_counter()\n",
    "                                    ann_res = ann.query(q)\n",
    "                                    qtime = time.perf_counter() - t0\n",
    "                                    query_times.append(qtime)\n",
    "                                    if ann_res is not None:\n",
    "                                        if np.linalg.norm(ann_res - q) <= (1+eps)*r:\n",
    "                                            successes += 1\n",
    "\n",
    "                            success_rate = (\n",
    "                                successes / valid_queries if valid_queries > 0 else None\n",
    "                            )\n",
    "\n",
    "                            record = {\n",
    "                                \"r\": r,\n",
    "                                \"epsilon\": eps,\n",
    "                                \"eta\": eta,\n",
    "                                \"n\": n,\n",
    "                                \"repeat\": rep,\n",
    "                                \"success_rate\": success_rate,\n",
    "                                \"avg_query_time_ms\": np.mean(query_times)*1000 if query_times else None,\n",
    "                                \"mem_usage_mb\": mem_ann_mb,\n",
    "                                \"stored_points\": len(ann.points),\n",
    "                                \"valid_queries\": valid_queries\n",
    "                            }\n",
    "\n",
    "                            # After you're done using ann\n",
    "                            del ann\n",
    "                            gc.collect()\n",
    "                            results_buffer.append(record)\n",
    "                            results.append(record)\n",
    "\n",
    "                            # Periodic checkpoint to file\n",
    "                            if len(results_buffer) >= checkpoint_every:\n",
    "                                for rec in results_buffer:\n",
    "                                    f.write(json.dumps(rec) + \"\\n\")\n",
    "                                f.flush()\n",
    "                                os.fsync(f.fileno())\n",
    "                                results_buffer = []\n",
    "\n",
    "                            pbar.update(1)\n",
    "\n",
    "    # Write remaining results\n",
    "    for rec in results_buffer:\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "    f.flush()\n",
    "    f.close()\n",
    "\n",
    "    print(f\"Experiments finished. Results logged to {log_file}.\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594954ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 completed runs in streaming_ann_log.jsonl, skipping them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "StreamingANN experiments:   0%|          | 0/8 [00:00<?, ?it/s, r=1, eps=0.2, eta=0.2, n=1000, rep=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Init] Inputs : eta=0.20, epsilon=0.20, r=1.00\n",
      "[Init] LSH Parameters : w=2.785, p1=0.7141, p2=0.6591, rho=0.8080\n",
      "[Init] Data Structure Parameters : k=17, L=371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "StreamingANN experiments:  12%|█▎        | 1/8 [03:11<22:18, 191.18s/it, r=1, eps=0.2, eta=0.2, n=1e+4, rep=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Init] Inputs : eta=0.20, epsilon=0.20, r=1.00\n",
      "[Init] LSH Parameters : w=2.785, p1=0.7141, p2=0.6591, rho=0.8080\n",
      "[Init] Data Structure Parameters : k=23, L=2389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "StreamingANN experiments:  25%|██▌       | 2/8 [35:08<2:00:39, 1206.60s/it, r=1, eps=0.2, eta=0.2, n=3e+4, rep=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Init] Inputs : eta=0.20, epsilon=0.20, r=1.00\n",
      "[Init] LSH Parameters : w=2.785, p1=0.7141, p2=0.6591, rho=0.8080\n",
      "[Init] Data Structure Parameters : k=25, L=5806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "StreamingANN experiments:  38%|███▊      | 3/8 [2:17:32<4:48:24, 3460.95s/it, r=1, eps=0.2, eta=0.2, n=5e+4, rep=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Init] Inputs : eta=0.20, epsilon=0.20, r=1.00\n",
      "[Init] LSH Parameters : w=2.785, p1=0.7141, p2=0.6591, rho=0.8080\n",
      "[Init] Data Structure Parameters : k=26, L=8773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "StreamingANN experiments:  38%|███▊      | 3/8 [38:07:00<63:31:41, 45740.22s/it, r=1, eps=0.2, eta=0.2, n=5e+4, rep=0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      8\u001b[39m n_list    = [\u001b[32m1000\u001b[39m, \u001b[32m10000\u001b[39m, \u001b[32m30000\u001b[39m, \u001b[32m50000\u001b[39m]  \u001b[38;5;66;03m# number of points from stream\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Run experiments\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mevaluate_streaming_ann\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnn_dict\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mloaded_nn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mr_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mr_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43meta_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43meta_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepeats\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_queries\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_file\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstreaming_ann_log.jsonl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     24\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# 4. Save results to CSV\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------\u001b[39;00m\n\u001b[32m     29\u001b[39m df = pd.DataFrame(results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 91\u001b[39m, in \u001b[36mevaluate_streaming_ann\u001b[39m\u001b[34m(data, nn_dict, r_list, eps_list, eta_list, n_list, repeats, n_queries, seed, log_file, checkpoint_every)\u001b[39m\n\u001b[32m     88\u001b[39m ann = StreamingANN(d=data.shape[\u001b[32m1\u001b[39m], n_estimate=n,\n\u001b[32m     89\u001b[39m                    eta=eta, epsilon=eps, r=r)\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[43mann\u001b[49m\u001b[43m.\u001b[49m\u001b[43minsert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# --- memory snapshot after ANN build ---\u001b[39;00m\n\u001b[32m     94\u001b[39m mem_after = proc.memory_info().rss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vedch\\Desktop\\Streaming-ANN-and-Sliding-Window-KDE\\Code\\StreamingANN\\StreamingANN.py:136\u001b[39m, in \u001b[36mStreamingANN.insert\u001b[39m\u001b[34m(self, point)\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28mself\u001b[39m.points.append(point)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.L):\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     key = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhash_functions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    138\u001b[39m         \u001b[38;5;28mself\u001b[39m.dropped_points += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vedch\\Desktop\\Streaming-ANN-and-Sliding-Window-KDE\\Code\\StreamingANN\\StreamingANN.py:112\u001b[39m, in \u001b[36mStreamingANN._make_key\u001b[39m\u001b[34m(self, point, hash_funcs)\u001b[39m\n\u001b[32m    109\u001b[39m offset = max_val // \u001b[32m2\u001b[39m   \u001b[38;5;66;03m# center around 0\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (a, b) \u001b[38;5;129;01min\u001b[39;00m hash_funcs:\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     val = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_h\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m     val_shifted = val + offset\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m val_shifted < \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m val_shifted > max_val:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vedch\\Desktop\\Streaming-ANN-and-Sliding-Window-KDE\\Code\\StreamingANN\\StreamingANN.py:99\u001b[39m, in \u001b[36mStreamingANN._h\u001b[39m\u001b[34m(self, v, a, b)\u001b[39m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_h\u001b[39m(\u001b[38;5;28mself\u001b[39m, v, a, b):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Single p-stable hash h_{a,b}(v).\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(np.floor((\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m + b) / \u001b[38;5;28mself\u001b[39m.w))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------\n",
    "# Define experiment parameter ranges\n",
    "# Tune these ranges based on your dataset size and dimension\n",
    "# --------------------------------------------------\n",
    "r_list    = [1.0, 1.25, 1.5]       # candidate radii\n",
    "eps_list  = [0.1, 0.2, 0.5, 1]         # epsilon values\n",
    "eta_list  = [0.0, 0.1, 0.2, 0.5]     # sampling aggressiveness\n",
    "n_list    = [1000, 10000, 30000, 50000]  # number of points from stream\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Run experiments\n",
    "# --------------------------------------------------\n",
    "results = evaluate_streaming_ann(\n",
    "    data,\n",
    "    nn_dict = loaded_nn_dict,\n",
    "    r_list=r_list,\n",
    "    eps_list=eps_list,\n",
    "    eta_list=eta_list,\n",
    "    n_list=n_list,\n",
    "    repeats=1,\n",
    "    n_queries=10000,\n",
    "    checkpoint_every=1,\n",
    "    log_file=\"streaming_ann_log.jsonl\"\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Save results to CSV\n",
    "# --------------------------------------------------\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"streaming_ann_results.csv\", index=False)\n",
    "print(\"Saved results to streaming_ann_results.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07950e36",
   "metadata": {},
   "source": [
    "### Evaluating JL Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efaa9229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_completed_jl_params(log_file):\n",
    "    \"\"\"Load completed (c, r, delta, n, rep) tuples from a JSONL log file.\"\"\"\n",
    "    done = set()\n",
    "    if os.path.exists(log_file):\n",
    "        with open(log_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    rec = json.loads(line)\n",
    "                    key = (rec[\"c\"], rec[\"r\"], rec[\"delta\"], rec[\"n\"], rec[\"repeat\"])\n",
    "                    done.add(key)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    return done\n",
    "\n",
    "\n",
    "def evaluate_streaming_jl(\n",
    "    data, nn_dict, c_list, r_list, delta_list, n_list,\n",
    "    repeats=3, n_queries=10000, seed=42,\n",
    "    log_file=\"streaming_jl_log.jsonl\", checkpoint_every=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Run StreamingJL experiments with periodic checkpointing and resume support.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.ndarray\n",
    "        Dataset array (N,d)\n",
    "    nn_dict : dict\n",
    "        Maps each vector index in data to its nearest neighbor\n",
    "    c_list, r_list, delta_list, n_list : list\n",
    "        Parameter ranges\n",
    "    repeats : int\n",
    "        Number of repeats per configuration\n",
    "    n_queries : int\n",
    "        Number of queries per run\n",
    "    log_file : str\n",
    "        Path to JSONL log file for checkpointing\n",
    "    checkpoint_every : int\n",
    "        Write to disk after every X results\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    proc = psutil.Process(os.getpid())\n",
    "    results_buffer = []\n",
    "\n",
    "    completed = load_completed_jl_params(log_file)\n",
    "    print(f\"Found {len(completed)} completed runs in {log_file}, skipping them.\")\n",
    "\n",
    "    f = open(log_file, \"a\")\n",
    "    total_runs = len(c_list) * len(r_list) * len(delta_list) * len(n_list) * repeats\n",
    "\n",
    "    with tqdm(total=total_runs, desc=\"StreamingJL experiments\") as pbar:\n",
    "        for c in c_list:\n",
    "            for r in r_list:\n",
    "                for delta in delta_list:\n",
    "                    for n in n_list:\n",
    "                        if n > len(data):\n",
    "                            continue\n",
    "                        for rep in range(repeats):\n",
    "                            key = (c, r, delta, n, rep)\n",
    "                            if key in completed:\n",
    "                                pbar.update(1)\n",
    "                                continue\n",
    "\n",
    "                            pbar.set_postfix({\"c\": c, \"r\": r, \"delta\": delta, \"n\": n, \"rep\": rep})\n",
    "\n",
    "                            # --- dataset and query split ---\n",
    "                            idx = rng.choice(len(data), size=n, replace=False)\n",
    "                            dataset = data[idx]\n",
    "                            remaining_idx = list(set(range(len(data))) - set(idx))\n",
    "                            q_count = min(n_queries, len(remaining_idx))\n",
    "                            q_idx = rng.choice(remaining_idx, size=q_count, replace=False)\n",
    "                            queries = data[q_idx]\n",
    "\n",
    "                            # --- memory before JL ---\n",
    "                            mem_before = proc.memory_info().rss\n",
    "\n",
    "                            # Build StreamingJL\n",
    "                            jl_ann = StreamingJL(d=data.shape[1], n_max=n, c=c, r=r, delta=delta, random_state=seed)\n",
    "                            for i, p in enumerate(dataset):\n",
    "                                jl_ann.insert(p, i)\n",
    "\n",
    "                            mem_after = proc.memory_info().rss\n",
    "                            mem_jl_mb = (mem_after - mem_before) / 1024**2\n",
    "\n",
    "                            # --- query evaluation ---\n",
    "                            successes = 0\n",
    "                            valid_queries = 0\n",
    "                            query_times = []\n",
    "\n",
    "                            for q_global_idx, q in zip(q_idx, queries):\n",
    "                                nn_idx = nn_dict[q_global_idx]\n",
    "                                nn_point = data[nn_idx]\n",
    "\n",
    "                                if np.linalg.norm(nn_point - q) <= r:\n",
    "                                    valid_queries += 1\n",
    "                                    t0 = time.perf_counter()\n",
    "                                    ans_id = jl_ann.query(q)\n",
    "                                    qtime = time.perf_counter() - t0\n",
    "                                    query_times.append(qtime)\n",
    "                                    if ans_id is not None:\n",
    "                                        dist = np.linalg.norm(dataset[ans_id] - q)\n",
    "                                        if dist <= c * r:\n",
    "                                            successes += 1\n",
    "\n",
    "                            success_rate = (successes / valid_queries) if valid_queries > 0 else None\n",
    "\n",
    "                            record = {\n",
    "                                \"c\": c,\n",
    "                                \"r\": r,\n",
    "                                \"delta\": delta,\n",
    "                                \"n\": n,\n",
    "                                \"repeat\": rep,\n",
    "                                \"success_rate\": success_rate,\n",
    "                                \"avg_query_time_ms\": np.mean(query_times) * 1000 if query_times else None,\n",
    "                                \"mem_usage_mb\": mem_jl_mb,\n",
    "                                \"stored_points\": len(jl_ann.points),\n",
    "                                \"valid_queries\": valid_queries\n",
    "                            }\n",
    "\n",
    "                            del jl_ann\n",
    "                            gc.collect()\n",
    "                            results_buffer.append(record)\n",
    "\n",
    "                            if len(results_buffer) >= checkpoint_every:\n",
    "                                for rec in results_buffer:\n",
    "                                    f.write(json.dumps(rec) + \"\\n\")\n",
    "                                f.flush()\n",
    "                                os.fsync(f.fileno())\n",
    "                                results_buffer = []\n",
    "\n",
    "                            pbar.update(1)\n",
    "\n",
    "    # Write remaining results\n",
    "    for rec in results_buffer:\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "    f.flush()\n",
    "    f.close()\n",
    "\n",
    "    print(f\"JL experiments finished. Results logged to {log_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b99fae41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 completed runs in streaming_jl_log.jsonl, skipping them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "StreamingJL experiments:  25%|██▌       | 1/4 [3:18:53<9:56:39, 11933.06s/it, c=1.2, r=1, delta=0.1, n=1e+4, rep=0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Run experiments\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m     24\u001b[39m jl_results_file = \u001b[33m\"\u001b[39m\u001b[33mstreaming_jl_log.jsonl\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[43mevaluate_streaming_jl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloaded_nn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mc_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mc_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mr_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mr_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelta_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdelta_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepeats\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_queries\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_every\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjl_results_file\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Load results from log file\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m     42\u001b[39m jl_results = []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mevaluate_streaming_jl\u001b[39m\u001b[34m(data, nn_dict, c_list, r_list, delta_list, n_list, repeats, n_queries, seed, log_file, checkpoint_every)\u001b[39m\n\u001b[32m     95\u001b[39m valid_queries += \u001b[32m1\u001b[39m\n\u001b[32m     96\u001b[39m t0 = time.perf_counter()\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m ans_id = \u001b[43mjl_ann\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m qtime = time.perf_counter() - t0\n\u001b[32m     99\u001b[39m query_times.append(qtime)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vedch\\Desktop\\Streaming-ANN-and-Sliding-Window-KDE\\Code\\StreamingANN\\JL_baseline.py:42\u001b[39m, in \u001b[36mStreamingJL.query\u001b[39m\u001b[34m(self, q)\u001b[39m\n\u001b[32m     40\u001b[39m best_id, best_dist = \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mfloat\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33minf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pid, p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.ids, \u001b[38;5;28mself\u001b[39m.points):\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     d = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzq\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m d < best_dist \u001b[38;5;129;01mand\u001b[39;00m d <= (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.eps) * \u001b[38;5;28mself\u001b[39m.c * \u001b[38;5;28mself\u001b[39m.r:\n\u001b[32m     44\u001b[39m         best_id, best_dist = pid, d\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vedch\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2791\u001b[39m, in \u001b[36mnorm\u001b[39m\u001b[34m(x, ord, axis, keepdims)\u001b[39m\n\u001b[32m   2789\u001b[39m     sqnorm = x_real.dot(x_real) + x_imag.dot(x_imag)\n\u001b[32m   2790\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2791\u001b[39m     sqnorm = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2792\u001b[39m ret = sqrt(sqnorm)\n\u001b[32m   2793\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keepdims:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Make sure your StreamingJL and JLTransformer classes are already defined\n",
    "# Also make sure evaluate_streaming_jl and load_completed_jl_params are defined\n",
    "\n",
    "# ------------------------------\n",
    "# Experiment parameter ranges\n",
    "# ------------------------------\n",
    "c_list     = [1.2]                # JL scaling factor\n",
    "r_list     = [1.0]                # candidate radii\n",
    "delta_list = [0.1]                # JL failure probability\n",
    "n_list     = [1000, 10000, 30000, 50000]  # number of points from stream\n",
    "\n",
    "# ------------------------------\n",
    "# Run experiments\n",
    "# ------------------------------\n",
    "jl_results_file = \"streaming_jl_log.jsonl\"\n",
    "\n",
    "evaluate_streaming_jl(\n",
    "    data,\n",
    "    nn_dict=loaded_nn_dict,\n",
    "    c_list=c_list,\n",
    "    r_list=r_list,\n",
    "    delta_list=delta_list,\n",
    "    n_list=n_list,\n",
    "    repeats=1,\n",
    "    n_queries=10000,\n",
    "    checkpoint_every=1,\n",
    "    log_file=jl_results_file\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# Load results from log file\n",
    "# ------------------------------\n",
    "jl_results = []\n",
    "with open(jl_results_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            jl_results.append(json.loads(line))\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "# ------------------------------\n",
    "# Save results to CSV\n",
    "# ------------------------------\n",
    "df_jl = pd.DataFrame(jl_results)\n",
    "df_jl.to_csv(\"streaming_jl_results.csv\", index=False)\n",
    "print(\"Saved results to streaming_jl_results.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae6f121",
   "metadata": {},
   "source": [
    "### QPS vs Recall Benchmarking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f029b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c64822cf",
   "metadata": {},
   "source": [
    "### Memory vs Recall Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce2cf10",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
